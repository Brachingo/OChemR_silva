[03/31 12:43:44] detectron2 INFO: Rank of current process: 1. World size: 2
[03/31 12:43:45] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.2
detectron2              0.6 @/u/markmartori/detectron2/detectron2
Compiler                GCC 8.5
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.2 @/dccstor/arrow_backup/conda-envs/detectron2/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1                 Tesla V100-SXM2-32GB (arch=7.0)
Driver version          510.39.01
CUDA_HOME               /usr/local/cuda
Pillow                  8.4.0
torchvision             0.11.3 @/dccstor/arrow_backup/conda-envs/detectron2/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220119
iopath                  0.1.9
cv2                     4.5.5
----------------------  -------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[03/31 12:43:45] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_256_6_6_torchvision.yaml', dist_url='tcp://127.0.0.1:61206', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[03/31 12:43:45] detectron2 INFO: Contents of args.config_file=configs/detr_256_6_6_torchvision.yaml:
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mDetr[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/dccstor/arrow_backup/detr-r50_ready_to_train_conv.pth[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m247.171[39m[38;5;15m,[39m[38;5;15m246.534[39m[38;5;15m,[39m[38;5;15m247.171[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m35.266[39m[38;5;15m,[39m[38;5;15m35.725[39m[38;5;15m [39m[38;5;15m,[39m[38;5;15m35.292[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mDETR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mGIOU_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mL1_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("my_dataset_train",)[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("my_dataset_val",)[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(369600,)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m554400[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mADAMW[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mfull_model[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(480,[39m[38;5;141m [39m[38;5;141m512,[39m[38;5;141m [39m[38;5;141m544,[39m[38;5;141m [39m[38;5;141m576,[39m[38;5;141m [39m[38;5;141m608,[39m[38;5;141m [39m[38;5;141m640,[39m[38;5;141m [39m[38;5;141m672,[39m[38;5;141m [39m[38;5;141m704,[39m[38;5;141m [39m[38;5;141m736,[39m[38;5;141m [39m[38;5;141m768,[39m[38;5;141m [39m[38;5;141m800)[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mabsolute_range[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(384,[39m[38;5;141m [39m[38;5;141m600)[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mRGB[39m[38;5;186m"[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m

[03/31 12:43:45] detectron2.utils.env INFO: Using a generated random seed 46527448
[03/31 12:43:48] detectron2.engine.defaults INFO: Model:
Detr(
  (detr): DETR(
    (transformer): Transformer(
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (class_embed): Linear(in_features=256, out_features=6, bias=True)
    (bbox_embed): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embed): Embedding(100, 256)
    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionEmbeddingSine()
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[03/31 12:44:04] detectron2.data.datasets.coco INFO: Loading /dccstor/arrow_backup/images/json_annotation.json takes 16.35 seconds.
[03/31 12:44:05] detectron2.data.datasets.coco INFO: Loaded 80000 images in COCO format from /dccstor/arrow_backup/images/json_annotation.json
[03/31 12:44:16] detectron2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   molec    | 696390       |   arrow    | 462717       |    text    | 1496514      |
|    plus    | 57803        |            |              |            |              |
|   total    | 2713424      |            |              |            |              |[0m
[03/31 12:44:16] detectron2.data.build INFO: Using training sampler TrainingSampler
[03/31 12:44:16] detectron2.data.common INFO: Serializing 80000 elements to byte tensors and concatenating them all ...
[03/31 12:44:17] detectron2.data.common INFO: Serialized dataset takes 168.07 MiB
[03/31 12:44:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /dccstor/arrow_backup/weights/DocSegTr_pretrained.pth ...
[03/31 12:44:25] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.stem.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.stem.conv1.weight[0m
[34mdetr.bbox_embed.layers.0.{bias, weight}[0m
[34mdetr.bbox_embed.layers.1.{bias, weight}[0m
[34mdetr.bbox_embed.layers.2.{bias, weight}[0m
[34mdetr.class_embed.{bias, weight}[0m
[34mdetr.input_proj.{bias, weight}[0m
[34mdetr.query_embed.weight[0m
[34mdetr.transformer.decoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.norm.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[03/31 12:44:25] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.stem.conv1.weight[0m
  [35mbackbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv1.weight[0m
  [35mbackbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv2.weight[0m
  [35mbackbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv3.weight[0m
  [35mbackbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv1.weight[0m
  [35mbackbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv2.weight[0m
  [35mbackbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv3.weight[0m
  [35mbackbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv1.weight[0m
  [35mbackbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv2.weight[0m
  [35mbackbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv3.weight[0m
  [35mbackbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv1.weight[0m
  [35mbackbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv2.weight[0m
  [35mbackbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv3.weight[0m
  [35mbackbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv1.weight[0m
  [35mbackbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv2.weight[0m
  [35mbackbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv3.weight[0m
  [35mbackbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv1.weight[0m
  [35mbackbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv2.weight[0m
  [35mbackbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv3.weight[0m
  [35mbackbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv1.weight[0m
  [35mbackbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv2.weight[0m
  [35mbackbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv3.weight[0m
  [35mbackbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv1.weight[0m
  [35mbackbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv2.weight[0m
  [35mbackbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv3.weight[0m
  [35mbackbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv1.weight[0m
  [35mbackbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv2.weight[0m
  [35mbackbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv3.weight[0m
  [35mbackbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv1.weight[0m
  [35mbackbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv2.weight[0m
  [35mbackbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv3.weight[0m
  [35mbackbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv1.weight[0m
  [35mbackbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv2.weight[0m
  [35mbackbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv3.weight[0m
  [35mbackbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv1.weight[0m
  [35mbackbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv2.weight[0m
  [35mbackbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv3.weight[0m
  [35mbackbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv1.weight[0m
  [35mbackbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv2.weight[0m
  [35mbackbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv3.weight[0m
  [35mbackbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv1.weight[0m
  [35mbackbone.bottom_up.res4.6.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv2.weight[0m
  [35mbackbone.bottom_up.res4.6.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv3.weight[0m
  [35mbackbone.bottom_up.res4.6.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv1.weight[0m
  [35mbackbone.bottom_up.res4.7.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv2.weight[0m
  [35mbackbone.bottom_up.res4.7.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv3.weight[0m
  [35mbackbone.bottom_up.res4.7.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv1.weight[0m
  [35mbackbone.bottom_up.res4.8.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv2.weight[0m
  [35mbackbone.bottom_up.res4.8.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv3.weight[0m
  [35mbackbone.bottom_up.res4.8.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv1.weight[0m
  [35mbackbone.bottom_up.res4.9.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv2.weight[0m
  [35mbackbone.bottom_up.res4.9.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv3.weight[0m
  [35mbackbone.bottom_up.res4.9.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv1.weight[0m
  [35mbackbone.bottom_up.res4.10.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv2.weight[0m
  [35mbackbone.bottom_up.res4.10.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv3.weight[0m
  [35mbackbone.bottom_up.res4.10.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv1.weight[0m
  [35mbackbone.bottom_up.res4.11.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv2.weight[0m
  [35mbackbone.bottom_up.res4.11.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv3.weight[0m
  [35mbackbone.bottom_up.res4.11.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv1.weight[0m
  [35mbackbone.bottom_up.res4.12.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv2.weight[0m
  [35mbackbone.bottom_up.res4.12.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv3.weight[0m
  [35mbackbone.bottom_up.res4.12.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv1.weight[0m
  [35mbackbone.bottom_up.res4.13.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv2.weight[0m
  [35mbackbone.bottom_up.res4.13.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv3.weight[0m
  [35mbackbone.bottom_up.res4.13.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv1.weight[0m
  [35mbackbone.bottom_up.res4.14.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv2.weight[0m
  [35mbackbone.bottom_up.res4.14.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv3.weight[0m
  [35mbackbone.bottom_up.res4.14.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv1.weight[0m
  [35mbackbone.bottom_up.res4.15.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv2.weight[0m
  [35mbackbone.bottom_up.res4.15.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv3.weight[0m
  [35mbackbone.bottom_up.res4.15.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv1.weight[0m
  [35mbackbone.bottom_up.res4.16.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv2.weight[0m
  [35mbackbone.bottom_up.res4.16.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv3.weight[0m
  [35mbackbone.bottom_up.res4.16.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv1.weight[0m
  [35mbackbone.bottom_up.res4.17.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv2.weight[0m
  [35mbackbone.bottom_up.res4.17.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv3.weight[0m
  [35mbackbone.bottom_up.res4.17.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv1.weight[0m
  [35mbackbone.bottom_up.res4.18.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv2.weight[0m
  [35mbackbone.bottom_up.res4.18.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv3.weight[0m
  [35mbackbone.bottom_up.res4.18.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv1.weight[0m
  [35mbackbone.bottom_up.res4.19.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv2.weight[0m
  [35mbackbone.bottom_up.res4.19.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv3.weight[0m
  [35mbackbone.bottom_up.res4.19.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv1.weight[0m
  [35mbackbone.bottom_up.res4.20.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv2.weight[0m
  [35mbackbone.bottom_up.res4.20.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv3.weight[0m
  [35mbackbone.bottom_up.res4.20.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv1.weight[0m
  [35mbackbone.bottom_up.res4.21.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv2.weight[0m
  [35mbackbone.bottom_up.res4.21.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv3.weight[0m
  [35mbackbone.bottom_up.res4.21.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv1.weight[0m
  [35mbackbone.bottom_up.res4.22.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv2.weight[0m
  [35mbackbone.bottom_up.res4.22.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv3.weight[0m
  [35mbackbone.bottom_up.res4.22.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv1.weight[0m
  [35mbackbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv2.weight[0m
  [35mbackbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv3.weight[0m
  [35mbackbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv1.weight[0m
  [35mbackbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv2.weight[0m
  [35mbackbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv3.weight[0m
  [35mbackbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv1.weight[0m
  [35mbackbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv2.weight[0m
  [35mbackbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv3.weight[0m
  [35mbackbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.0.g[0m
  [35mcate_head.transformer.layers.blocks.1.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.1.g[0m
  [35mcate_head.transformer.layers.blocks.1.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.0.g[0m
  [35mcate_head.transformer.layers.blocks.3.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.1.g[0m
  [35mcate_head.transformer.layers.blocks.3.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.0.g[0m
  [35mcate_head.transformer.layers.blocks.5.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.1.g[0m
  [35mcate_head.transformer.layers.blocks.5.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.0.g[0m
  [35mcate_head.transformer.layers.blocks.7.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.1.g[0m
  [35mcate_head.transformer.layers.blocks.7.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.0.g[0m
  [35mcate_head.transformer.layers.blocks.9.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.1.g[0m
  [35mcate_head.transformer.layers.blocks.9.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.0.g[0m
  [35mcate_head.transformer.layers.blocks.11.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.1.g[0m
  [35mcate_head.transformer.layers.blocks.11.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.0.g[0m
  [35mcate_head.transformer.layers.blocks.13.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.1.g[0m
  [35mcate_head.transformer.layers.blocks.13.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.0.g[0m
  [35mcate_head.transformer.layers.blocks.15.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.1.g[0m
  [35mcate_head.transformer.layers.blocks.15.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.0.g[0m
  [35mcate_head.transformer.layers.blocks.17.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.1.g[0m
  [35mcate_head.transformer.layers.blocks.17.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.0.g[0m
  [35mcate_head.transformer.layers.blocks.19.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.1.g[0m
  [35mcate_head.transformer.layers.blocks.19.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.0.g[0m
  [35mcate_head.transformer.layers.blocks.21.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.1.g[0m
  [35mcate_head.transformer.layers.blocks.21.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.0.g[0m
  [35mcate_head.transformer.layers.blocks.23.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.1.g[0m
  [35mcate_head.transformer.layers.blocks.23.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.1.fn.2.{bias, weight}[0m
  [35mcate_head.cate_pred.{bias, weight}[0m
  [35mcate_head.kernel_pred.{bias, weight}[0m
  [35mmask_head.convs_all_levels.0.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.0.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.1.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.1.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.2.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.2.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.2.conv1.0.weight[0m
  [35mmask_head.convs_all_levels.2.conv1.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv1.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv1.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv2.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv2.1.{bias, weight}[0m
  [35mmask_head.conv_pred.0.weight[0m
  [35mmask_head.conv_pred.1.{bias, weight}[0m
[03/31 12:44:25] detectron2.engine.train_loop INFO: Starting training from iteration 0
[03/31 12:46:02] detectron2.engine.hooks INFO: Overall training speed: 148 iterations in 0:00:47 (0.3226 s / it)
[03/31 12:46:02] detectron2.engine.hooks INFO: Total training time: 0:00:47 (0:00:00 on hooks)
[03/31 12:47:30] detectron2 INFO: Rank of current process: 1. World size: 2
[03/31 12:47:31] detectron2 INFO: Environment info:
----------------------  -------------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.2
detectron2              0.6 @/u/markmartori/detectron2/detectron2
Compiler                GCC 8.5
CUDA compiler           not available
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.2 @/dccstor/arrow_backup/conda-envs/detectron2/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1                 Tesla V100-SXM2-32GB (arch=7.0)
Driver version          510.39.01
CUDA_HOME               /usr/local/cuda
Pillow                  8.4.0
torchvision             0.11.3 @/dccstor/arrow_backup/conda-envs/detectron2/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220119
iopath                  0.1.9
cv2                     4.5.5
----------------------  -------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[03/31 12:47:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/detr_256_6_6_torchvision.yaml', dist_url='tcp://127.0.0.1:61206', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=[], resume=False)
[03/31 12:47:31] detectron2 INFO: Contents of args.config_file=configs/detr_256_6_6_torchvision.yaml:
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mDetr[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/dccstor/arrow_backup/weights/DocSegTr_pretrained.pth"[39m[38;5;15m [39m[38;5;242m#"/dccstor/arrow_backup/detr-r50_ready_to_train_conv.pth"[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m247.171[39m[38;5;15m,[39m[38;5;15m246.534[39m[38;5;15m,[39m[38;5;15m247.171[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m35.266[39m[38;5;15m,[39m[38;5;15m35.725[39m[38;5;15m [39m[38;5;15m,[39m[38;5;15m35.292[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mDETR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mGIOU_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mL1_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("my_dataset_train",)[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("my_dataset_val",)[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(369600,)[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m554400[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mADAMW[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mfull_model[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(480,[39m[38;5;141m [39m[38;5;141m512,[39m[38;5;141m [39m[38;5;141m544,[39m[38;5;141m [39m[38;5;141m576,[39m[38;5;141m [39m[38;5;141m608,[39m[38;5;141m [39m[38;5;141m640,[39m[38;5;141m [39m[38;5;141m672,[39m[38;5;141m [39m[38;5;141m704,[39m[38;5;141m [39m[38;5;141m736,[39m[38;5;141m [39m[38;5;141m768,[39m[38;5;141m [39m[38;5;141m800)[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mabsolute_range[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(384,[39m[38;5;141m [39m[38;5;141m600)[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mRGB[39m[38;5;186m"[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m

[03/31 12:47:31] detectron2.utils.env INFO: Using a generated random seed 32592797
[03/31 12:47:34] detectron2.engine.defaults INFO: Model:
Detr(
  (detr): DETR(
    (transformer): Transformer(
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (1): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (2): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (3): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (4): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
          (5): TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (class_embed): Linear(in_features=256, out_features=6, bias=True)
    (bbox_embed): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_embed): Embedding(100, 256)
    (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionEmbeddingSine()
    )
  )
  (criterion): SetCriterion(
    (matcher): HungarianMatcher()
  )
)
[03/31 12:47:50] detectron2.data.datasets.coco INFO: Loading /dccstor/arrow_backup/images/json_annotation.json takes 15.56 seconds.
[03/31 12:47:51] detectron2.data.datasets.coco INFO: Loaded 80000 images in COCO format from /dccstor/arrow_backup/images/json_annotation.json
[03/31 12:48:01] detectron2.data.build INFO: Distribution of instances among all 4 categories:
[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|
|   molec    | 696390       |   arrow    | 462717       |    text    | 1496514      |
|    plus    | 57803        |            |              |            |              |
|   total    | 2713424      |            |              |            |              |[0m
[03/31 12:48:01] detectron2.data.build INFO: Using training sampler TrainingSampler
[03/31 12:48:06] detectron2.data.common INFO: Serializing 80000 elements to byte tensors and concatenating them all ...
[03/31 12:48:07] detectron2.data.common INFO: Serialized dataset takes 168.07 MiB
[03/31 12:48:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /dccstor/arrow_backup/weights/DocSegTr_pretrained.pth ...
[03/31 12:48:10] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res2.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res3.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.3.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.4.conv3.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv1.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv2.weight[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res4.5.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.0.shortcut.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.1.conv3.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv1.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv2.weight[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.res5.2.conv3.weight[0m
[34mdetr.backbone.0.backbone.stem.conv1.norm.{bias, weight}[0m
[34mdetr.backbone.0.backbone.stem.conv1.weight[0m
[34mdetr.bbox_embed.layers.0.{bias, weight}[0m
[34mdetr.bbox_embed.layers.1.{bias, weight}[0m
[34mdetr.bbox_embed.layers.2.{bias, weight}[0m
[34mdetr.class_embed.{bias, weight}[0m
[34mdetr.input_proj.{bias, weight}[0m
[34mdetr.query_embed.weight[0m
[34mdetr.transformer.decoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.norm3.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.decoder.norm.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mdetr.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.out_proj.{bias, weight}[0m
[34mdetr.transformer.encoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[03/31 12:48:10] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.stem.conv1.weight[0m
  [35mbackbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv1.weight[0m
  [35mbackbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv2.weight[0m
  [35mbackbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.0.conv3.weight[0m
  [35mbackbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv1.weight[0m
  [35mbackbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv2.weight[0m
  [35mbackbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.1.conv3.weight[0m
  [35mbackbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv1.weight[0m
  [35mbackbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv2.weight[0m
  [35mbackbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res2.2.conv3.weight[0m
  [35mbackbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv1.weight[0m
  [35mbackbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv2.weight[0m
  [35mbackbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.0.conv3.weight[0m
  [35mbackbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv1.weight[0m
  [35mbackbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv2.weight[0m
  [35mbackbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.1.conv3.weight[0m
  [35mbackbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv1.weight[0m
  [35mbackbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv2.weight[0m
  [35mbackbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.2.conv3.weight[0m
  [35mbackbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv1.weight[0m
  [35mbackbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv2.weight[0m
  [35mbackbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res3.3.conv3.weight[0m
  [35mbackbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv1.weight[0m
  [35mbackbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv2.weight[0m
  [35mbackbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.0.conv3.weight[0m
  [35mbackbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv1.weight[0m
  [35mbackbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv2.weight[0m
  [35mbackbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.1.conv3.weight[0m
  [35mbackbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv1.weight[0m
  [35mbackbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv2.weight[0m
  [35mbackbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.2.conv3.weight[0m
  [35mbackbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv1.weight[0m
  [35mbackbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv2.weight[0m
  [35mbackbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.3.conv3.weight[0m
  [35mbackbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv1.weight[0m
  [35mbackbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv2.weight[0m
  [35mbackbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.4.conv3.weight[0m
  [35mbackbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv1.weight[0m
  [35mbackbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv2.weight[0m
  [35mbackbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.5.conv3.weight[0m
  [35mbackbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv1.weight[0m
  [35mbackbone.bottom_up.res4.6.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv2.weight[0m
  [35mbackbone.bottom_up.res4.6.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.6.conv3.weight[0m
  [35mbackbone.bottom_up.res4.6.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv1.weight[0m
  [35mbackbone.bottom_up.res4.7.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv2.weight[0m
  [35mbackbone.bottom_up.res4.7.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.7.conv3.weight[0m
  [35mbackbone.bottom_up.res4.7.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv1.weight[0m
  [35mbackbone.bottom_up.res4.8.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv2.weight[0m
  [35mbackbone.bottom_up.res4.8.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.8.conv3.weight[0m
  [35mbackbone.bottom_up.res4.8.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv1.weight[0m
  [35mbackbone.bottom_up.res4.9.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv2.weight[0m
  [35mbackbone.bottom_up.res4.9.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.9.conv3.weight[0m
  [35mbackbone.bottom_up.res4.9.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv1.weight[0m
  [35mbackbone.bottom_up.res4.10.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv2.weight[0m
  [35mbackbone.bottom_up.res4.10.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.10.conv3.weight[0m
  [35mbackbone.bottom_up.res4.10.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv1.weight[0m
  [35mbackbone.bottom_up.res4.11.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv2.weight[0m
  [35mbackbone.bottom_up.res4.11.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.11.conv3.weight[0m
  [35mbackbone.bottom_up.res4.11.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv1.weight[0m
  [35mbackbone.bottom_up.res4.12.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv2.weight[0m
  [35mbackbone.bottom_up.res4.12.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.12.conv3.weight[0m
  [35mbackbone.bottom_up.res4.12.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv1.weight[0m
  [35mbackbone.bottom_up.res4.13.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv2.weight[0m
  [35mbackbone.bottom_up.res4.13.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.13.conv3.weight[0m
  [35mbackbone.bottom_up.res4.13.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv1.weight[0m
  [35mbackbone.bottom_up.res4.14.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv2.weight[0m
  [35mbackbone.bottom_up.res4.14.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.14.conv3.weight[0m
  [35mbackbone.bottom_up.res4.14.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv1.weight[0m
  [35mbackbone.bottom_up.res4.15.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv2.weight[0m
  [35mbackbone.bottom_up.res4.15.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.15.conv3.weight[0m
  [35mbackbone.bottom_up.res4.15.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv1.weight[0m
  [35mbackbone.bottom_up.res4.16.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv2.weight[0m
  [35mbackbone.bottom_up.res4.16.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.16.conv3.weight[0m
  [35mbackbone.bottom_up.res4.16.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv1.weight[0m
  [35mbackbone.bottom_up.res4.17.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv2.weight[0m
  [35mbackbone.bottom_up.res4.17.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.17.conv3.weight[0m
  [35mbackbone.bottom_up.res4.17.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv1.weight[0m
  [35mbackbone.bottom_up.res4.18.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv2.weight[0m
  [35mbackbone.bottom_up.res4.18.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.18.conv3.weight[0m
  [35mbackbone.bottom_up.res4.18.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv1.weight[0m
  [35mbackbone.bottom_up.res4.19.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv2.weight[0m
  [35mbackbone.bottom_up.res4.19.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.19.conv3.weight[0m
  [35mbackbone.bottom_up.res4.19.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv1.weight[0m
  [35mbackbone.bottom_up.res4.20.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv2.weight[0m
  [35mbackbone.bottom_up.res4.20.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.20.conv3.weight[0m
  [35mbackbone.bottom_up.res4.20.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv1.weight[0m
  [35mbackbone.bottom_up.res4.21.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv2.weight[0m
  [35mbackbone.bottom_up.res4.21.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.21.conv3.weight[0m
  [35mbackbone.bottom_up.res4.21.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv1.weight[0m
  [35mbackbone.bottom_up.res4.22.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv2.weight[0m
  [35mbackbone.bottom_up.res4.22.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res4.22.conv3.weight[0m
  [35mbackbone.bottom_up.res4.22.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.shortcut.weight[0m
  [35mbackbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv1.weight[0m
  [35mbackbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv2.weight[0m
  [35mbackbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.0.conv3.weight[0m
  [35mbackbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv1.weight[0m
  [35mbackbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv2.weight[0m
  [35mbackbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.1.conv3.weight[0m
  [35mbackbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv1.weight[0m
  [35mbackbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv2_offset.{bias, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv2.weight[0m
  [35mbackbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}[0m
  [35mbackbone.bottom_up.res5.2.conv3.weight[0m
  [35mbackbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.0.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.0.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.0.g[0m
  [35mcate_head.transformer.layers.blocks.1.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.1.g[0m
  [35mcate_head.transformer.layers.blocks.1.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.1.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.2.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.2.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.0.g[0m
  [35mcate_head.transformer.layers.blocks.3.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.1.g[0m
  [35mcate_head.transformer.layers.blocks.3.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.3.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.4.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.4.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.0.g[0m
  [35mcate_head.transformer.layers.blocks.5.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.1.g[0m
  [35mcate_head.transformer.layers.blocks.5.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.5.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.6.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.6.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.0.g[0m
  [35mcate_head.transformer.layers.blocks.7.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.1.g[0m
  [35mcate_head.transformer.layers.blocks.7.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.7.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.8.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.8.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.0.g[0m
  [35mcate_head.transformer.layers.blocks.9.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.1.g[0m
  [35mcate_head.transformer.layers.blocks.9.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.9.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.10.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.10.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.0.g[0m
  [35mcate_head.transformer.layers.blocks.11.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.1.g[0m
  [35mcate_head.transformer.layers.blocks.11.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.11.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.12.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.12.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.0.g[0m
  [35mcate_head.transformer.layers.blocks.13.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.1.g[0m
  [35mcate_head.transformer.layers.blocks.13.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.13.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.14.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.14.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.0.g[0m
  [35mcate_head.transformer.layers.blocks.15.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.1.g[0m
  [35mcate_head.transformer.layers.blocks.15.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.15.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.16.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.16.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.0.g[0m
  [35mcate_head.transformer.layers.blocks.17.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.1.g[0m
  [35mcate_head.transformer.layers.blocks.17.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.17.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.18.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.18.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.0.g[0m
  [35mcate_head.transformer.layers.blocks.19.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.1.g[0m
  [35mcate_head.transformer.layers.blocks.19.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.19.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.20.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.20.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.0.g[0m
  [35mcate_head.transformer.layers.blocks.21.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.1.g[0m
  [35mcate_head.transformer.layers.blocks.21.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.21.1.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.g[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.22.0.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.g[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_q.weight[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_kv.weight[0m
  [35mcate_head.transformer.layers.blocks.22.1.fn.fn.to_out.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.0.g[0m
  [35mcate_head.transformer.layers.blocks.23.0.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.0.fn.2.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.1.g[0m
  [35mcate_head.transformer.layers.blocks.23.1.fn.0.{bias, weight}[0m
  [35mcate_head.transformer.layers.blocks.23.1.fn.2.{bias, weight}[0m
  [35mcate_head.cate_pred.{bias, weight}[0m
  [35mcate_head.kernel_pred.{bias, weight}[0m
  [35mmask_head.convs_all_levels.0.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.0.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.1.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.1.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.2.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.2.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.2.conv1.0.weight[0m
  [35mmask_head.convs_all_levels.2.conv1.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv0.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv0.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv1.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv1.1.{bias, weight}[0m
  [35mmask_head.convs_all_levels.3.conv2.0.weight[0m
  [35mmask_head.convs_all_levels.3.conv2.1.{bias, weight}[0m
  [35mmask_head.conv_pred.0.weight[0m
  [35mmask_head.conv_pred.1.{bias, weight}[0m
[03/31 12:48:10] detectron2.engine.train_loop INFO: Starting training from iteration 0
[03/31 12:49:42] detectron2.engine.hooks INFO: Overall training speed: 171 iterations in 0:00:56 (0.3292 s / it)
[03/31 12:49:42] detectron2.engine.hooks INFO: Total training time: 0:00:56 (0:00:00 on hooks)
